{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706f7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n",
    "                         wrap_fp16_model)\n",
    "\n",
    "from mmdet.apis import multi_gpu_test, single_gpu_test\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "from mmdet.models import build_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef32980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='MMDet test (and eval) a model')\n",
    "    parser.add_argument('config', default = 'configs/svhn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_svhn.py',help='test config file path')\n",
    "    parser.add_argument('--checkpoint',default = 'output/latest.pth', help='checkpoint file')\n",
    "    parser.add_argument(\n",
    "        '--work-dir',\n",
    "        help='the directory to save the file containing evaluation metrics')\n",
    "    parser.add_argument('--out', help='output result file in pickle format')\n",
    "    parser.add_argument(\n",
    "        '--fuse-conv-bn',\n",
    "        action='store_true',\n",
    "        help='Whether to fuse conv and bn, this will slightly increase'\n",
    "        'the inference speed')\n",
    "    parser.add_argument(\n",
    "        '--format-only',\n",
    "        action='store_true',\n",
    "        help='Format the output results without perform evaluation. It is'\n",
    "        'useful when you want to format the result to a specific format and '\n",
    "        'submit it to the test server')\n",
    "    parser.add_argument(\n",
    "        '--eval',\n",
    "        type=str,\n",
    "        nargs='+',\n",
    "        help='evaluation metrics, which depends on the dataset, e.g., \"bbox\",'\n",
    "        ' \"segm\", \"proposal\" for COCO, and \"mAP\", \"recall\" for PASCAL VOC')\n",
    "    parser.add_argument('--show', action='store_true', help='show results')\n",
    "    parser.add_argument(\n",
    "        '--show-dir', help='directory where painted images will be saved')\n",
    "    parser.add_argument(\n",
    "        '--show-score-thr',\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help='score threshold (default: 0.3)')\n",
    "    parser.add_argument(\n",
    "        '--gpu-collect',\n",
    "        action='store_true',\n",
    "        help='whether to use gpu to collect results.')\n",
    "    parser.add_argument(\n",
    "        '--tmpdir',\n",
    "        help='tmp directory used for collecting results from multiple '\n",
    "        'workers, available when gpu-collect is not specified')\n",
    "    parser.add_argument(\n",
    "        '--cfg-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='override some settings in the used config, the key-value pair '\n",
    "        'in xxx=yyy format will be merged into config file. If the value to '\n",
    "        'be overwritten is a list, it should be like key=\"[a,b]\" or key=a,b '\n",
    "        'It also allows nested list/tuple values, e.g. key=\"[(a,b),(c,d)]\" '\n",
    "        'Note that the quotation marks are necessary and that no white space '\n",
    "        'is allowed.')\n",
    "    parser.add_argument(\n",
    "        '--options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='custom options for evaluation, the key-value pair in xxx=yyy '\n",
    "        'format will be kwargs for dataset.evaluate() function (deprecate), '\n",
    "        'change to --eval-options instead.')\n",
    "    parser.add_argument(\n",
    "        '--eval-options',\n",
    "        nargs='+',\n",
    "        action=DictAction,\n",
    "        help='custom options for evaluation, the key-value pair in xxx=yyy '\n",
    "        'format will be kwargs for dataset.evaluate() function')\n",
    "    parser.add_argument(\n",
    "        '--launcher',\n",
    "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "        default='none',\n",
    "        help='job launcher')\n",
    "    parser.add_argument('--local_rank', type=int, default=0)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "    \n",
    "    if args.options and args.eval_options:\n",
    "        raise ValueError(\n",
    "            '--options and --eval-options cannot be both '\n",
    "            'specified, --options is deprecated in favor of --eval-options')\n",
    "    if args.options:\n",
    "        warnings.warn('--options is deprecated in favor of --eval-options')\n",
    "        args.eval_options = args.options\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ae28b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YaShu\\AppData\\Roaming\\jupyter\\runtime\\kernel-3271081d-5c1e-4c43-850f-a2c9487b8c2c.json\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "print(args.config)\n",
    "args.config = \"configs/svhn/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_3x_svhn.py\"\n",
    "args.checkpoint = \"output/latest.pth\"\n",
    "#args.eval = \"bbox\"\n",
    "args.out = \"out.pkl\"\n",
    "args.work_dir = \"output/testResult\"\n",
    "# args.show_dir = \"output/testResult/img\"\n",
    "args.format_only = True\n",
    "dictt=dict(jsonfile_prefix=\"output\")\n",
    "args.options = dictt\n",
    "args.eval_options = dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4354f0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YaShu\\Anaconda3\\envs\\openmmlab\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "C:\\Users\\YaShu\\Desktop\\google drive(nctu)\\python\\CBNetV2-main\\CBNetV2-main\\mmdet\\core\\anchor\\builder.py:16: UserWarning: ``build_anchor_generator`` would be deprecated soon, please use ``build_prior_generator`` \n",
      "  '``build_anchor_generator`` would be deprecated soon, please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: output/latest.pth\n",
      "[                                                  ] 0/13068, elapsed: 0s, ETA:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YaShu\\Desktop\\google drive(nctu)\\python\\CBNetV2-main\\CBNetV2-main\\mmdet\\core\\anchor\\anchor_generator.py:323: UserWarning: ``grid_anchors`` would be deprecated soon. Please use ``grid_priors`` \n",
      "  warnings.warn('``grid_anchors`` would be deprecated soon. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                                  ] 2/13068, 0.2 task/s, elapsed: 12s, ETA: 75417ss"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YaShu\\Desktop\\google drive(nctu)\\python\\CBNetV2-main\\CBNetV2-main\\mmdet\\core\\anchor\\anchor_generator.py:360: UserWarning: ``single_level_grid_anchors`` would be deprecated soon. Please use ``single_level_grid_priors`` \n",
      "  '``single_level_grid_anchors`` would be deprecated soon. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 13068/13068, 10.2 task/s, elapsed: 1284s, ETA:     0s\n",
      "writing results to out.pkl\n"
     ]
    }
   ],
   "source": [
    "assert args.out or args.eval or args.format_only or args.show \\\n",
    "    or args.show_dir, \\\n",
    "    ('Please specify at least one operation (save/eval/format/show the '\n",
    "     'results / save the results) with the argument \"--out\", \"--eval\"'\n",
    "     ', \"--format-only\", \"--show\" or \"--show-dir\"')\n",
    "if args.eval and args.format_only:\n",
    "    raise ValueError('--eval and --format_only cannot be both specified')\n",
    "if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n",
    "    raise ValueError('The output file must be a pkl file.')\n",
    "cfg = Config.fromfile(args.config)\n",
    "if args.cfg_options is not None:\n",
    "    cfg.merge_from_dict(args.cfg_options)\n",
    "# import modules from string list.\n",
    "if cfg.get('custom_imports', None):\n",
    "    from mmcv.utils import import_modules_from_strings\n",
    "    import_modules_from_strings(**cfg['custom_imports'])\n",
    "# set cudnn_benchmark\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "cfg.model.pretrained = None\n",
    "if cfg.model.get('neck'):\n",
    "    if isinstance(cfg.model.neck, list):\n",
    "        for neck_cfg in cfg.model.neck:\n",
    "            if neck_cfg.get('rfp_backbone'):\n",
    "                if neck_cfg.rfp_backbone.get('pretrained'):\n",
    "                    neck_cfg.rfp_backbone.pretrained = None\n",
    "    elif cfg.model.neck.get('rfp_backbone'):\n",
    "        if cfg.model.neck.rfp_backbone.get('pretrained'):\n",
    "            cfg.model.neck.rfp_backbone.pretrained = None\n",
    "# in case the test dataset is concatenated\n",
    "samples_per_gpu = 1\n",
    "if isinstance(cfg.data.test, dict):\n",
    "    cfg.data.test.test_mode = True\n",
    "    samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)\n",
    "    if samples_per_gpu > 1:\n",
    "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "        cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "            cfg.data.test.pipeline)\n",
    "elif isinstance(cfg.data.test, list):\n",
    "    for ds_cfg in cfg.data.test:\n",
    "        ds_cfg.test_mode = True\n",
    "    samples_per_gpu = max(\n",
    "        [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])\n",
    "    if samples_per_gpu > 1:\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "# init distributed env first, since logger depends on the dist info.\n",
    "if args.launcher == 'none':\n",
    "    distributed = False\n",
    "else:\n",
    "    distributed = True\n",
    "    init_dist(args.launcher, **cfg.dist_params)\n",
    "rank, _ = get_dist_info()\n",
    "# allows not to create\n",
    "if args.work_dir is not None and rank == 0:\n",
    "    mmcv.mkdir_or_exist(osp.abspath(args.work_dir))\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "    json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')\n",
    "# build the dataloader\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=samples_per_gpu,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=distributed,\n",
    "    shuffle=False)\n",
    "# build the model and load checkpoint\n",
    "cfg.model.train_cfg = None\n",
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "fp16_cfg = cfg.get('fp16', None)\n",
    "if fp16_cfg is not None:\n",
    "    wrap_fp16_model(model)\n",
    "checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')\n",
    "if args.fuse_conv_bn:\n",
    "    model = fuse_conv_bn(model)\n",
    "# old versions did not save class info in checkpoints, this walkaround is\n",
    "# for backward compatibility\n",
    "if 'CLASSES' in checkpoint.get('meta', {}):\n",
    "    model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "else:\n",
    "    model.CLASSES = dataset.CLASSES\n",
    "if not distributed:\n",
    "    model = MMDataParallel(model, device_ids=[0])\n",
    "    outputs = single_gpu_test(model, data_loader, args.show, args.show_dir,\n",
    "                              args.show_score_thr)\n",
    "else:\n",
    "    model = MMDistributedDataParallel(\n",
    "        model.cuda(),\n",
    "        device_ids=[torch.cuda.current_device()],\n",
    "        broadcast_buffers=False)\n",
    "    outputs = multi_gpu_test(model, data_loader, args.tmpdir,\n",
    "                             args.gpu_collect)\n",
    "rank, _ = get_dist_info()\n",
    "if rank == 0:\n",
    "    if args.out:\n",
    "        print(f'\\nwriting results to {args.out}')\n",
    "        mmcv.dump(outputs, args.out)\n",
    "    kwargs = {} if args.eval_options is None else args.eval_options\n",
    "    if args.format_only:\n",
    "        dataset.format_results(outputs, **kwargs)\n",
    "    if args.eval:\n",
    "        eval_kwargs = cfg.get('evaluation', {}).copy()\n",
    "        # hard-code way to remove EvalHook args\n",
    "        for key in [\n",
    "                'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',\n",
    "                'rule'\n",
    "        ]:\n",
    "            eval_kwargs.pop(key, None)\n",
    "        eval_kwargs.update(dict(metric=args.eval, **kwargs))\n",
    "        metric = dataset.evaluate(outputs, **eval_kwargs)\n",
    "        print(metric)\n",
    "        metric_dict = dict(config=args.config, metric=metric)\n",
    "        if args.work_dir is not None and rank == 0:\n",
    "            mmcv.dump(metric_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669910b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
